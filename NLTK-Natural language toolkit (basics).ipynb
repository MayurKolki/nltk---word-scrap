{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ce1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NLTK-Natural language toolkit\n",
    "2) stop words\n",
    "3) lemma\n",
    "4)POS tagging(parts-of-speech)\n",
    "5)ngram(1-gram,2-gram,3-gram...)\n",
    "6)word similarity\n",
    "7)TF-IDF(term-frequency / inverse-document frequency)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da50c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61660b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', '7', 'days', 'in', 'a', 'week', '.', 'Sunday', 'is', 'the', 'first', 'day', '.', 'Saturday', 'is', 'last', 'day', '.', 'I', 'hate', 'Mondays'] "
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "#break sentence into words(word tokenization)\n",
    "#convert text to words\n",
    "p1=\"There are 7 days in a week. Sunday is the first day . Saturday is last day. I hate Mondays\"\n",
    "wt = nltk.word_tokenize(p1)\n",
    "print(wt,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a145e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are 7 days in a week.', 'Sunday is the first day .', 'Saturday is last day.', 'I hate Mondays']\n"
     ]
    }
   ],
   "source": [
    "#convert text to sentence (sentence tokenization)\n",
    "st = nltk.sent_tokenize(p1)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722584fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', '7', 'days', 'in', 'a', 'week', '.']\n",
      "['Sunday', 'is', 'the', 'first', 'day', '.']\n",
      "['Saturday', 'is', 'last', 'day', '.']\n",
      "['I', 'hate', 'Mondays']\n"
     ]
    }
   ],
   "source": [
    "#word tokenzisation on the individual sentences\n",
    "for s in st:\n",
    "    print(nltk.word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57576130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'Dr.Kumar',\n",
       " '.',\n",
       " 'He',\n",
       " 'is',\n",
       " 'a',\n",
       " 'surgeon',\n",
       " 'with',\n",
       " 'over',\n",
       " '20',\n",
       " 'years',\n",
       " 'of',\n",
       " 'experience',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'his',\n",
       " 'close',\n",
       " 'Friend',\n",
       " 'Mr.Singh',\n",
       " '.',\n",
       " 'He',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Businessman']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dealing with abbreviations \n",
    "s1=\"This is Dr.Kumar. He is a surgeon with over 20 years of experience. This is his close Friend Mr.Singh. He is a Businessman\"\n",
    "nltk.word_tokenize(s1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812c5e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is Dr.Kumar.',\n",
       " 'He is a surgeon with over 20 years of experience.',\n",
       " 'This is his close Friend Mr.Singh.',\n",
       " 'He is a Businessman']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84cbc3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'do',\n",
       " 'this',\n",
       " 'its',\n",
       " 'to',\n",
       " 'complicated',\n",
       " '.',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'spend',\n",
       " 'too',\n",
       " 'much',\n",
       " 'time',\n",
       " '.',\n",
       " 'It',\n",
       " 'ai',\n",
       " \"n't\",\n",
       " 'worth',\n",
       " 'the',\n",
       " 'effort',\n",
       " 'and',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'interest',\n",
       " 'me']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dealing with apostrophes \n",
    "s2 = \"I can't do this its to complicated . I don't want to spend too much time. It ain't worth the effort and doesn't interest me\"\n",
    "nltk.word_tokenize(s2) #does not split  word properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34378a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I can't do this its to complicated .\",\n",
       " \"I don't want to spend too much time.\",\n",
       " \"It ain't worth the effort and doesn't interest me\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4401bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "pattern = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50d2c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"can't\",\n",
       " 'do',\n",
       " 'this',\n",
       " 'its',\n",
       " 'to',\n",
       " 'complicated',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'spend',\n",
       " 'too',\n",
       " 'much',\n",
       " 'time',\n",
       " 'It',\n",
       " \"ain't\",\n",
       " 'worth',\n",
       " 'the',\n",
       " 'effort',\n",
       " 'and',\n",
       " \"doesn't\",\n",
       " 'interest',\n",
       " 'me']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa611d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#2) stop words \n",
    "\"the movie was good . it had a nice storyline with a good cast. apart from the songs, overall it was an enjoyable movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df829323",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the/was/it/had/a/with/a/apart/from/the/overall/it/was/an   ->stock words\n",
    "movie/good/nice/storyline/good/cast/songs/enjoyable\n",
    "\n",
    "we can create our own stock words\n",
    "paisa\n",
    "judgaad\n",
    "khalaas\n",
    "pani\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dd96537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us create your own stcok words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "049062fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aren', 'until', 'were', 'each', 'himself', 'a', 'themselves', 'to', 'the', 'am', 'at', 'mightn', 'through', 'because', \"you're\", 'ain', 'under', 'd', 'on', \"weren't\", 'can', \"needn't\", 'hers', \"don't\", 'too', \"doesn't\", 'some', 'yours', 'with', \"mustn't\", 'above', 'what', 'myself', 'and', 'how', 'out', 'should', 'my', 't', 'did', 'both', \"won't\", 'below', 'why', \"it's\", 'be', 'during', 'couldn', 'does', \"didn't\", 'has', \"aren't\", 'where', 'll', 'these', 'hasn', 'itself', 'is', 'weren', \"wouldn't\", 'his', 'you', 'once', 'wasn', 'other', 'down', 'yourself', 've', \"she's\", 'no', 'most', 'for', \"you'll\", 'just', 'isn', 'few', \"you'd\", \"couldn't\", \"shouldn't\", \"isn't\", 'have', 'mustn', 'been', 'do', 'she', 'so', 'whom', 'm', 'needn', 'into', 'there', 'theirs', 'here', 'further', 'between', \"shan't\", 'now', 'while', 'from', 'they', 'those', 'that', 'its', 'this', \"haven't\", 'again', 'will', \"you've\", 'nor', 'such', 'very', 'me', 'about', 'yourselves', 'than', \"mightn't\", \"wasn't\", 'was', 'which', 'as', 'hadn', 'don', 'up', 'shouldn', 'all', 'it', 'before', 'by', 'more', 'doing', \"hasn't\", 'any', 'haven', 'we', 'won', 'ma', 'if', \"that'll\", 'our', 'having', 'didn', 're', 'had', 'or', \"hadn't\", 'after', 'when', 'shan', 'but', 'doesn', 'o', 'in', 'off', 'an', 'same', 'herself', 'y', 'i', 'who', 'he', 'of', 'are', 'ourselves', 'wouldn', 'him', 'against', 'ours', 'then', \"should've\", 'her', 'being', 'only', 'not', 'them', 'own', 'your', 'their', 's', 'over'}\n"
     ]
    }
   ],
   "source": [
    "#get the default list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76331b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new words can be appende to the stop words\n",
    "#applicable till the kernel is running\n",
    "stop_words.update({'jugaad','rs.','masala','.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17588508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'jugaad',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'masala',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 'rs.',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "093cecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie was full of masala and was not worth 100 rs. the direction was bad and most stunts looked like jugaad. overall a bad movie  \n"
     ]
    }
   ],
   "source": [
    "s3 =\"the movie was full of masala and was not worth 100 rs. the direction was bad and most stunts looked like jugaad. overall a bad movie  \"\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5347b851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'movie',\n",
       " 'was',\n",
       " 'full',\n",
       " 'of',\n",
       " 'masala',\n",
       " 'and',\n",
       " 'was',\n",
       " 'not',\n",
       " 'worth',\n",
       " '100',\n",
       " 'rs',\n",
       " '.',\n",
       " 'the',\n",
       " 'direction',\n",
       " 'was',\n",
       " 'bad',\n",
       " 'and',\n",
       " 'most',\n",
       " 'stunts',\n",
       " 'looked',\n",
       " 'like',\n",
       " 'jugaad',\n",
       " '.',\n",
       " 'overall',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'movie']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tokenization of the sentence\n",
    "words = nltk.word_tokenize(s3)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcd15cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'full', 'worth', '100', 'rs', 'direction', 'bad', 'stunts', 'looked', 'like', 'overall', 'bad', 'movie']\n"
     ]
    }
   ],
   "source": [
    "#reformat the above sentence agter removing the stopwords\n",
    "s3_new = [w for w in words if w not in stop_words]\n",
    "print(s3_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f70970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie full worth 100 rs direction bad stunts looked like overall bad movie'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(s3_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "s1 = this is good      -->p\n",
    "s2 = this is not good  -->n\n",
    "\n",
    "is good --> p\n",
    "not good -->n\n",
    "---------------------------------------------------------------------\n",
    "ram and shyam are friends. ram has a big house. he is a nice person. shyam visits ram often. he lives far away \n",
    "\n",
    "which he belonds to whom\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dab2d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lemma  ->every word found in a dictionary \n",
    "#hypernym -> word having a broder meaning(COLOR -> hypernym for RED)\n",
    "#hyponym  -> general meaning (SPOON -> hyponym for CUTLERY)\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cf1c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2168870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('machine.n.01'),\n",
       " Synset('machine.n.02'),\n",
       " Synset('machine.n.03'),\n",
       " Synset('machine.n.04'),\n",
       " Synset('machine.n.05'),\n",
       " Synset('car.n.01'),\n",
       " Synset('machine.v.01'),\n",
       " Synset('machine.v.02')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = wn.synsets(word)\n",
    "res\n",
    "#n -> noun\n",
    "#v -> verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8f73580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'any mechanical or electrical device that transmits or modifies energy to perform or assist in the performance of human tasks'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#definition \n",
    "res[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19419808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an efficient person'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32abfe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an intricate organization that accomplishes its goals efficiently'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[2].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bb58283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a device for overcoming resistance at one point by applying force at some other point'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[3].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5f59bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a group that controls the activities of a political party'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[4].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61377af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the boxer was a magnificent fighting machine']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example \n",
    "res[1].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31f5aa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the war machine']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[2].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56004723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he was endorsed by the Democratic machine']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[4].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9275679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: any mechanical or electrical device that transmits or modifies energy to perform or assist in the performance of human tasks\n",
      "Example :[]\n",
      "\n",
      "\n",
      "Definition: an efficient person\n",
      "Example :['the boxer was a magnificent fighting machine']\n",
      "\n",
      "\n",
      "Definition: an intricate organization that accomplishes its goals efficiently\n",
      "Example :['the war machine']\n",
      "\n",
      "\n",
      "Definition: a device for overcoming resistance at one point by applying force at some other point\n",
      "Example :[]\n",
      "\n",
      "\n",
      "Definition: a group that controls the activities of a political party\n",
      "Example :['he was endorsed by the Democratic machine']\n",
      "\n",
      "\n",
      "Definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Example :['he needs a car to get to work']\n",
      "\n",
      "\n",
      "Definition: turn, shape, mold, or otherwise finish by machinery\n",
      "Example :[]\n",
      "\n",
      "\n",
      "Definition: make by machinery\n",
      "Example :['The Americans were machining while others still hand-made cars']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print definitions and example of all lemmas of all lemmas returned for the word\n",
    "for i in range(len(res)):\n",
    "    print(\"Definition: {}\".format(res[i].definition()))\n",
    "    print('Example :{}'.format(res[i].examples()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8e458fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('machine.n.01.machine')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af5a2251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('machine.n.02.machine')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72c2252c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('machine.n.04.machine'), Lemma('machine.n.04.simple_machine')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[3].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cc1c7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('device.n.01')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hypernyms\n",
    "res[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbb65071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('assembly.n.01'),\n",
       " Synset('bagger.n.02'),\n",
       " Synset('calculator.n.02'),\n",
       " Synset('calender.n.01'),\n",
       " Synset('cash_machine.n.01'),\n",
       " Synset('comber.n.03'),\n",
       " Synset('computer.n.01'),\n",
       " Synset('concrete_mixer.n.01'),\n",
       " Synset('corker.n.02'),\n",
       " Synset('cotton_gin.n.01'),\n",
       " Synset('decoder.n.02'),\n",
       " Synset('farm_machine.n.01'),\n",
       " Synset('franking_machine.n.01'),\n",
       " Synset('hop-picker.n.01'),\n",
       " Synset('machine_tool.n.01'),\n",
       " Synset('machinery.n.01'),\n",
       " Synset('milking_machine.n.01'),\n",
       " Synset('motor.n.01'),\n",
       " Synset('pavior.n.01'),\n",
       " Synset('perpetual_motion_machine.n.01'),\n",
       " Synset('pile_driver.n.01'),\n",
       " Synset('power_shovel.n.01'),\n",
       " Synset('power_tool.n.01'),\n",
       " Synset('press.n.03'),\n",
       " Synset('press.n.07'),\n",
       " Synset('printer.n.03'),\n",
       " Synset('record_player.n.01'),\n",
       " Synset('riveting_machine.n.01'),\n",
       " Synset('self-feeder.n.01'),\n",
       " Synset('simulator.n.01'),\n",
       " Synset('slicer.n.02'),\n",
       " Synset('slot_machine.n.01'),\n",
       " Synset('snow_thrower.n.01'),\n",
       " Synset('sorter.n.02'),\n",
       " Synset('stamp.n.07'),\n",
       " Synset('staple_gun.n.01'),\n",
       " Synset('stapler.n.01'),\n",
       " Synset('textile_machine.n.01'),\n",
       " Synset('time_machine.n.01'),\n",
       " Synset('trimmer.n.02'),\n",
       " Synset('workhorse.n.01'),\n",
       " Synset('zamboni.n.01')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyponyms\n",
    "res[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23555d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cat.n.01')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### \n",
    "#word similarity (simiarity between 2 words )\n",
    "#returns number 0-1\n",
    "#close to 1 -> high similarity\n",
    "#close to 0 -> less similarity\n",
    "cat = wn.synsets('cat')[0]\n",
    "cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab879c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wn.synsets('dog')[0]\n",
    "dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba313a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the similarity between 'cat' and 'dog'\n",
    "dog.wup_similarity(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ex\n",
    "if >75% :\n",
    "    <> -> extract from data fetch on screen\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a2cdc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.Kumar',\n",
       " 'and',\n",
       " 'his',\n",
       " 'wife',\n",
       " 'Seema',\n",
       " 'set',\n",
       " 'up',\n",
       " 'this',\n",
       " 'hospital',\n",
       " '5',\n",
       " 'years',\n",
       " 'back',\n",
       " '.',\n",
       " 'They',\n",
       " 'built',\n",
       " 'this',\n",
       " 'when',\n",
       " 'they',\n",
       " 'had',\n",
       " 'limited',\n",
       " 'funds',\n",
       " 'and',\n",
       " 'got',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'support',\n",
       " 'from',\n",
       " 'family',\n",
       " 'and',\n",
       " 'friends']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS tagging (Parts-of- speech tagging )\n",
    "#every word in this sentence will be assigned a POS-tag\n",
    "s4 = 'Dr.Kumar and his wife Seema set up this hospital 5 years back. They built this when they had limited funds and got lots of support from family and friends '\n",
    "\n",
    "#before POS, do stop\n",
    "words = nltk.word_tokenize(s4)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "423e4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr.Kumar', 'NN'), ('and', 'CC'), ('his', 'PRP$'), ('wife', 'NN'), ('Seema', 'NNP'), ('set', 'VBD'), ('up', 'RP'), ('this', 'DT'), ('hospital', 'NN'), ('5', 'CD'), ('years', 'NNS'), ('back', 'RB'), ('.', '.'), ('They', 'PRP'), ('built', 'VBD'), ('this', 'DT'), ('when', 'WRB'), ('they', 'PRP'), ('had', 'VBD'), ('limited', 'VBN'), ('funds', 'NNS'), ('and', 'CC'), ('got', 'VBD'), ('lots', 'NNS'), ('of', 'IN'), ('support', 'NN'), ('from', 'IN'), ('family', 'NN'), ('and', 'CC'), ('friends', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "#apply POS  on the words\n",
    "pos_words = nltk.pos_tag(words)\n",
    "print(pos_words)  #output is list of tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. CC Coordinating conjunction\n",
    "2. CD Cardinal number\n",
    "3. DT Determiner\n",
    "4. EX Existential there\n",
    "5. FW Foreign word\n",
    "6. IN Preposition or subordinating conjunction\n",
    "7. JJ Adjective\n",
    "8. JJR Adjective, comparative\n",
    "9. JJS Adjective, superlative\n",
    "10. LS List item marker\n",
    "11. MD Modal\n",
    "12. NN Noun, singular or mass\n",
    "13. NNS Noun, plural\n",
    "14. NNP Proper noun, singular\n",
    "15. NNPS Proper noun, plural\n",
    "16. PDT Predeterminer\n",
    "17. POS Possessive ending\n",
    "18. PRP Personal pronoun\n",
    "19. PRP$ Possessive pronoun\n",
    "20. RB Adverb\n",
    "21. RBR Adverb, comparative\n",
    "22. RBS Adverb, superlative\n",
    "23. RP Particle\n",
    "24. SYM Symbol\n",
    "25. TO to\n",
    "26. UH Interjection\n",
    "27. VB Verb, base form\n",
    "28. VBD Verb, past tense\n",
    "29. VBG Verb, gerund or present participle\n",
    "30. VBN Verb, past participle\n",
    "31. VBP Verb, non-3rd person singular present\n",
    "32. VBZ Verb, 3rd person singular present\n",
    "33. WDT Wh-determiner\n",
    "34. WP Wh-pronoun\n",
    "35. WP$ Possessive wh-pronoun\n",
    "36. WRB Wh-adverb\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00d22e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.Kumar NN\n",
      "and CC\n",
      "his PRP$\n",
      "wife NN\n",
      "Seema NNP\n",
      "set VBD\n",
      "up RP\n",
      "this DT\n",
      "hospital NN\n",
      "5 CD\n",
      "years NNS\n",
      "back RB\n",
      ". .\n",
      "They PRP\n",
      "built VBD\n",
      "this DT\n",
      "when WRB\n",
      "they PRP\n",
      "had VBD\n",
      "limited VBN\n",
      "funds NNS\n",
      "and CC\n",
      "got VBD\n",
      "lots NNS\n",
      "of IN\n",
      "support NN\n",
      "from IN\n",
      "family NN\n",
      "and CC\n",
      "friends NNS\n"
     ]
    }
   ],
   "source": [
    "#get all the Noun from the sentence \n",
    "nouns = []\n",
    "for word,pos in pos_words:\n",
    "    print(word,pos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f33a7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "del nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35ed52c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dr.Kumar', 'and', 'his', 'wife', 'Seema', 'set', 'up', 'this', 'hospital', '5', 'years', 'back', '.', 'They', 'built', 'this', 'when', 'they', 'had', 'limited', 'funds', 'and', 'got', 'lots', 'of', 'support', 'from', 'family', 'and', 'friends'], ['Dr.Kumar', 'and', 'his', 'wife', 'Seema', 'set', 'up', 'this', 'hospital', '5', 'years', 'back', '.', 'They', 'built', 'this', 'when', 'they', 'had', 'limited', 'funds', 'and', 'got', 'lots', 'of', 'support', 'from', 'family', 'and', 'friends'], ['Dr.Kumar', 'and', 'his', 'wife', 'Seema', 'set', 'up', 'this', 'hospital', '5', 'years', 'back', '.', 'They', 'built', 'this', 'when', 'they', 'had', 'limited', 'funds', 'and', 'got', 'lots', 'of', 'support', 'from', 'family', 'and', 'friends'], ['Dr.Kumar', 'and', 'his', 'wife', 'Seema', 'set', 'up', 'this', 'hospital', '5', 'years', 'back', '.', 'They', 'built', 'this', 'when', 'they', 'had', 'limited', 'funds', 'and', 'got', 'lots', 'of', 'support', 'from', 'family', 'and', 'friends'], ['Dr.Kumar', 'and', 'his', 'wife', 'Seema', 'set', 'up', 'this', 'hospital', '5', 'years', 'back', '.', 'They', 'built', 'this', 'when', 'they', 'had', 'limited', 'funds', 'and', 'got', 'lots', 'of', 'support', 'from', 'family', 'and', 'friends'], ['Dr.Kumar', 'and', 'his', 'wife', 'Seema', 'set', 'up', 'this', 'hospital', '5', 'years', 'back', '.', 'They', 'built', 'this', 'when', 'they', 'had', 'limited', 'funds', 'and', 'got', 'lots', 'of', 'support', 'from', 'family', 'and', 'friends']]\n"
     ]
    }
   ],
   "source": [
    "nouns = [words for word,pos in pos_words if pos in ['NN','NMS','NNP','NNPS']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-grams \n",
    "'''\n",
    "\"i like sweets, fruits, juice and drinks\"\n",
    "\n",
    "n -> +ve integer\n",
    "n=1, i like sweets friuts juice drinks\n",
    "n=2, [i like],[like sweets],[sweets fruits],[fruits juice] [juice drinks]\n",
    "n=3, [i like sweets], [ like sweets] [sweets fruits juice] [fruits juice drinks]\n",
    "------------------------------------------------------------------------\n",
    "i like movies                 --> P\n",
    "i do not like movies          --> N\n",
    "\n",
    "he does not like to study     --> N  \n",
    "  \n",
    "[not like]  \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de769794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\mayur\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\mayur\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from textblob) (3.6.2)\n",
      "Requirement already satisfied: click in c:\\users\\mayur\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\mayur\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\mayur\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mayur\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.1->textblob) (4.60.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22b25b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n- grams\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53148c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['today']),\n",
       " WordList(['is']),\n",
       " WordList(['friday']),\n",
       " WordList(['and']),\n",
       " WordList(['beginning']),\n",
       " WordList(['of']),\n",
       " WordList(['the']),\n",
       " WordList(['weekend'])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s5 = \"today is friday and beginning of the weekend \"\n",
    "s5_blob = TextBlob(s5)\n",
    "\n",
    "#form the n-grams \n",
    "w1 = s5_blob.ngrams(1)\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2dcdef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['today', 'is']),\n",
       " WordList(['is', 'friday']),\n",
       " WordList(['friday', 'and']),\n",
       " WordList(['and', 'beginning']),\n",
       " WordList(['beginning', 'of']),\n",
       " WordList(['of', 'the']),\n",
       " WordList(['the', 'weekend'])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = s5_blob.ngrams(2)\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb8d675e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['today', 'is', 'friday']),\n",
       " WordList(['is', 'friday', 'and']),\n",
       " WordList(['friday', 'and', 'beginning']),\n",
       " WordList(['and', 'beginning', 'of']),\n",
       " WordList(['beginning', 'of', 'the']),\n",
       " WordList(['of', 'the', 'weekend'])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3 = s5_blob.ngrams(3)\n",
    "w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e64349df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['today', 'is', 'friday', 'and']),\n",
       " WordList(['is', 'friday', 'and', 'beginning']),\n",
       " WordList(['friday', 'and', 'beginning', 'of']),\n",
       " WordList(['and', 'beginning', 'of', 'the']),\n",
       " WordList(['beginning', 'of', 'the', 'weekend'])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w4 = s5_blob.ngrams(4)\n",
    "w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "28cc9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to find the closest matched word (edit- distance )\n",
    "'''\n",
    "monday chances are  -->  mnday, mondy\n",
    "'''\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f2fac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'today is friday and beginning of the weekend '"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0be8583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'is', 'friday', 'and', 'beginning', 'of', 'the', 'weekend']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(s5)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb8f3fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friday']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the word that amtches the word that is searched\n",
    "difflib.get_close_matches('frdy',words) #closest word matching 'frdy'=> ' friday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e403e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'weekend']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difflib.get_close_matches('end',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "767d3ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'friday']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difflib.get_close_matches('day',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5937f51",
   "metadata": {},
   "source": [
    "# TF-IDF(term-frequency / inverse-document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " TF-IDF  --> find the  importance of the given word\n",
    "\n",
    "let say we have a document\n",
    "and i have alist of document \n",
    "\n",
    "word  ->'python'\n",
    "it will relevance in doc and d1 and d2 d3 d3\n",
    "and multiple  they \n",
    "\n",
    "doc\n",
    "[]      [] [] [] []\n",
    "        d1 d2 d3 d4\n",
    " \n",
    "term frequency and inverse document \n",
    "\n",
    "50           Julia --> chances will be less\n",
    "resume        SQL  --> chances will be MORE\n",
    "            \n",
    "3 functions\n",
    "1) TF\n",
    "2)IDF\n",
    "3)TF.IDF\n",
    " \n",
    "'''\n",
    "todat is friday    --> 1/3  = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "135b62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb57df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)  TF\n",
    "def tf(word,doc):\n",
    "    #total count of word/total number words\n",
    "    token = nltk.word_tokenize(doc)\n",
    "    len_tokens = len(token)\n",
    "    word_count = doc.count(word)\n",
    "    \n",
    "    tf1 = word_count / len_tokens\n",
    "    \n",
    "    return(tf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b527f62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'is', 'friday', 'and', 'weekend']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'friday'\n",
    "doc = 'today is friday and weekend'\n",
    "words = nltk.word_tokenize(doc)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10117809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'today friday weekend'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [w for w in words if w not in stop_words]\n",
    "doc = ' '.join(doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "646efafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(word,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aee70f",
   "metadata": {},
   "source": [
    "## inverse-document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8d06fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word,list_of_doc):\n",
    "    \n",
    "    word_count = 0\n",
    "    \n",
    "    for d in list_of_doc:\n",
    "        if d.count(word) > 0:\n",
    "            word_count+=1\n",
    "            \n",
    "    #if the word count is 0\n",
    "    if word_count ==0:\n",
    "        word_count=1\n",
    "        \n",
    "      \n",
    "    #document count\n",
    "    doc_count = len(list_of_doc)\n",
    "    \n",
    "    #IDF \n",
    "    idf1 = math.log(doc_count / word_count)\n",
    "    \n",
    "    return(idf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a25d2cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"weekend\"\n",
    "\n",
    "d1 = 'today is friday and a weekend'\n",
    "d2 = 'i like the weekend and it is time to relax'\n",
    "d3 = 'saturday and sunday is for relaxing '\n",
    "\n",
    "#[d1,d2,d3]\n",
    " #  d1 ->0\n",
    "d1.count(word) +d2.count(word) +d3.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d6055cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today is friday and a weekend',\n",
       " 'i like the weekend and it is time to relax',\n",
       " 'saturday and sunday is for relaxing ']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_doc = [d1,d2,d3]\n",
    "len(list_of_doc)\n",
    "list_of_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8a3f86c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4054651081081644"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf(word,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine TF and IDF to form a single function TF-TDF (TF*IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05a4bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(word,doc,list_of_doc):\n",
    "    return(tf(word,doc)*idf(word,list_of_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6892d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"weekend\"\n",
    "\n",
    "d1 = 'today is friday and a weekend'\n",
    "d2 = 'i like the weekend and it is time to relax'\n",
    "d3 = 'saturday and sunday is for relaxing '\n",
    "list_of_doc = [d1,d2,d3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "80a03cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06757751801802739"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(word,d1,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "91feb210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04054651081081644"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(word,d2,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7b3b52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(word,d3,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to the TF-IDF after removing the stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4400be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assignment\n",
    "#create 4 different doc related to 'data science'\n",
    "'''\n",
    "1)pytho\n",
    "2)tableau\n",
    "3)algorithm\n",
    "\n",
    "do the stopwords implementation before finding the relevance\n",
    "'''\n",
    "\n",
    "d5  =\"Data scientist with a passion to solve real-world business challenges using data analytics \"\n",
    "d6  = \"Aspiring Data Scientist and Software Developer, Proficient in Data Science with focus on Feature Engineering\"\n",
    "d7  = \"Data Preprocessing ,Data wrangling and Python programming language\"\n",
    "d8  = \"Proficient in complex machine learning and statistical modeling algorithms/techniques for identifying patterns and extracting valuable insights for key stakeholders and organizational leadership. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "edb4014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'scientist',\n",
       " 'with',\n",
       " 'a',\n",
       " 'passion',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'real-world',\n",
       " 'business',\n",
       " 'challenges',\n",
       " 'using',\n",
       " 'data',\n",
       " 'analytics']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = nltk.word_tokenize(d5)\n",
    "word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f1306448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'scientist', 'passion', 'solve', 'real-world', 'business', 'challenges', 'using', 'data', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "#reformat the above sentence agter removing the stopwords\n",
    "d5_new = [w for w in word1 if w not in stop_words]\n",
    "print(d5_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "efa6883f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data scientist passion solve real-world business challenges using data analytics'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d5 = ' '.join(d5_new)\n",
    "d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c9150b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aspiring',\n",
       " 'Data',\n",
       " 'Scientist',\n",
       " 'and',\n",
       " 'Software',\n",
       " 'Developer',\n",
       " ',',\n",
       " 'Proficient',\n",
       " 'in',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'with',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'Feature',\n",
       " 'Engineering']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2 = nltk.word_tokenize(d6)\n",
    "word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a7e1e730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aspiring', 'Data', 'Scientist', 'and', 'Software', 'Developer', ',', 'Proficient', 'in', 'Data', 'Science', 'with', 'focus', 'on', 'Feature', 'Engineering']\n",
      "['Aspiring', 'Data', 'Scientist', 'Software', 'Developer', ',', 'Proficient', 'Data', 'Science', 'focus', 'Feature', 'Engineering']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Aspiring Data Scientist Software Developer , Proficient Data Science focus Feature Engineering'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2 = nltk.word_tokenize(d6)\n",
    "print(word2)\n",
    "#reformat the above sentence agter removing the stopwords\n",
    "d6_new = [w for w in word2 if w not in stop_words]\n",
    "print(d6_new)\n",
    "d6 = ' '.join(d6_new)\n",
    "d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85eed81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'Preprocessing',\n",
       " ',',\n",
       " 'Data',\n",
       " 'wrangling',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'programming',\n",
       " 'language']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word3 = nltk.word_tokenize(d7)\n",
    "word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6e64b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'Preprocessing', ',', 'Data', 'wrangling', 'and', 'Python', 'programming', 'language']\n",
      "['Data', 'Preprocessing', ',', 'Data', 'wrangling', 'Python', 'programming', 'language']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data Preprocessing , Data wrangling Python programming language'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word3 = nltk.word_tokenize(d7)\n",
    "print(word3)\n",
    "#reformat the above sentence agter removing the stopwords\n",
    "d7_new = [w for w in word3 if w not in stop_words]\n",
    "print(d7_new)\n",
    "d7 = ' '.join(d7_new)\n",
    "d7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "899b236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Proficient',\n",
       " 'in',\n",
       " 'complex',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'statistical',\n",
       " 'modeling',\n",
       " 'algorithms/techniques',\n",
       " 'for',\n",
       " 'identifying',\n",
       " 'patterns',\n",
       " 'and',\n",
       " 'extracting',\n",
       " 'valuable',\n",
       " 'insights',\n",
       " 'for',\n",
       " 'key',\n",
       " 'stakeholders',\n",
       " 'and',\n",
       " 'organizational',\n",
       " 'leadership',\n",
       " '.']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word4 = nltk.word_tokenize(d8)\n",
    "word4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c00ac363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Proficient', 'in', 'complex', 'machine', 'learning', 'and', 'statistical', 'modeling', 'algorithms/techniques', 'for', 'identifying', 'patterns', 'and', 'extracting', 'valuable', 'insights', 'for', 'key', 'stakeholders', 'and', 'organizational', 'leadership', '.']\n",
      "['Proficient', 'complex', 'machine', 'learning', 'statistical', 'modeling', 'algorithms/techniques', 'identifying', 'patterns', 'extracting', 'valuable', 'insights', 'key', 'stakeholders', 'organizational', 'leadership']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Proficient complex machine learning statistical modeling algorithms/techniques identifying patterns extracting valuable insights key stakeholders organizational leadership'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word4 = nltk.word_tokenize(d8)\n",
    "print(word4)\n",
    "#reformat the above sentence agter removing the stopwords\n",
    "d8_new = [w for w in word4 if w not in stop_words]\n",
    "print(d8_new)\n",
    "d8 = ' '.join(d8_new)\n",
    "d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2f95204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_doc = [d5,d6,d7,d8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b216e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word =\"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "484b51d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028768207245178087"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(word,d5,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "284a02f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04794701207529681"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(word,d6,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "600bf35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07192051811294521"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(word,d7,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cca7ad1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(word,d8,list_of_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2c334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
